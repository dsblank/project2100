{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Science Behind Project2100 Art Installation\n",
    "\n",
    "*This is a Jupyter Notebook. It contains text, pictures, data, and code relating to the Project2100 art installation at UMASS Dartmouth, Oct 13, 2016. The code for the project lives at https://github.com/dsblank/project2100*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Background\n",
    "\n",
    "In the summer of 2016, Rhine Singleton, Biologist at [Franklin Pierce University](https://www.franklinpierce.edu/), began discussing an idea with Douglas Blank, Computer Scientist at [Bryn Mawr College](https://www.brynmawr). Rhine had built programs before to turn climate change data into a audiotory experience. This is called *sonification*. But Rhine wanted to take the idea of sonification of climate change data and turn it into an art exhibit where participants could experience the sonification. A plan began to emerge: \n",
    "\n",
    "Create an art installation that:\n",
    "\n",
    "1. Allowed the participant to experience past climate change effects\n",
    "2. Allowed the participant to explore the possible outcomes\n",
    "3. Based the experience on scientific priniciples\n",
    "4. Created an engaging, aesthetic experience for the participant\n",
    "5. Used open-source code, allowing others to build on our work\n",
    "\n",
    "Five main ideas emerged from this meeting:\n",
    "\n",
    "1. Participants would walk through a *soundfield* which somehow represented time\n",
    "2. Participants would be able to pick future pathways resulting in different outcomes in the year 2100\n",
    "3. We would use sometype of sensor to detect their movement in space \n",
    "4. Sounds would be generated on-the-fly\n",
    "5. Video could also be generated on the fly, but focus would be on sound\n",
    "\n",
    "We began some initial digging and decided to explore:\n",
    "\n",
    "* chuck - a Python music generation interface originally by Ananya Misra based on [Princeton's music research](http://chuck.stanford.edu/)\n",
    "* kinect - the X-Box game sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Climate Change Data\n",
    "\n",
    "Original data from NASA: [nasa data 1898 to 2015.csv](nasa data 1898 to 2015.csv)\n",
    "\n",
    "The data was \"smoothed\" between 1950 and 2015. The variability in the raw data make the temp changes hard to parse. Though the variability in visual graphs and non-interactive versions of the sound graphs is generally nice, it was problematic here. \n",
    "\n",
    "You can see the final data here: https://github.com/dsblank/project2100/blob/master/project2100.py#L35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sound Space Interaction\n",
    "\n",
    "Rhine imagined the soundfield interaction zone to look something like this:\n",
    "\n",
    "<img src=\"20160725_123943_resized.jpg\" width=\"300\"/>\n",
    "\n",
    "Rhine surveyed possible areas inside the installation site, and made some videos of himself walking through these spaces:\n",
    "\n",
    "<img src=\"longnarrow.png\"/>\n",
    "\n",
    "Initial Kinect experimentation looked promising. Here is the first test image at Doug's house from the summer of 2016:\n",
    "\n",
    "<img src=\"test1.jpg\"/>\n",
    "\n",
    "The size of the test area was quite a bit different from Rhine's original larger soundfield space. However, Rhine began thinking about the layout of an adapted plan:\n",
    "\n",
    "<img src=\"Screenshot%20from%202016-10-09%2006%3A46%3A59.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we began to build the soundfield, we encountered many questions:\n",
    "\n",
    "* how can we give minimal instructions, but make it relay some information?\n",
    "* how can we make artful, and aesthetically pleasing?\n",
    "* how can we make the code run fast enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Build\n",
    "\n",
    "<img src=\"IMG_20161009_084912156.jpg\" width=\"600\" />\n",
    "<img src=\"IMG_20161009_224933258.jpg\" width=\"600\" />\n",
    "\n",
    "# The Results\n",
    "\n",
    "When a participant begins, the visual colors are blue-ish, the tempo is slow, and the frequency matches the temperatures from this time (lower):\n",
    "\n",
    "<img src=\"IMG_20161009_224716420.jpg\" width=\"600\" />\n",
    "\n",
    "If move you forward and to the right, the colors become yellow-sih, the tempo speeds up, and the frequency is higher:\n",
    "\n",
    "<img src=\"IMG_20161009_224837750.jpg\" width=\"600\" />\n",
    "\n",
    "If move you forward and to the left, the colors become red-sih, the tempo speeds up, and the frequency is higher:\n",
    "\n",
    "<img src=\"IMG_20161009_224749566.jpg\" width=\"600\" />\n",
    "<img src=\"closeup.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems Found and Worked-around\n",
    "\n",
    "* the data from the kinect was different from what was expected\n",
    "* finding robust parameters for detecting people's location was tricky\n",
    "* initializing chuck twice corrupts the sounds\n",
    "* an original bug had us working with a single row of pixels\n",
    "* auto-startup was very finicky; trial and error with delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lessons Learned\n",
    "\n",
    "* We should have built the graphical display earlier, for debugging purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the signs we created to hang with the installation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soundfield\n",
    "### 2016\n",
    "\n",
    "### Computer generated sounds and images\n",
    "\n",
    "Rhine Singleton  \n",
    "Associate Professor of Biology  \n",
    "Franklin Pierce University  \n",
    "\n",
    "Douglas Blank  \n",
    "Associate Professor of Computer Science  \n",
    "Bryn Mawr College  \n",
    "\n",
    "*What if we could hear the sound of temperature?*\n",
    "\n",
    "*What if we could walk into the future, or backwards through time?*\n",
    "\n",
    "*What if we continue, “business as usual?”*\n",
    "\n",
    "*What if we change our trajectory?*\n",
    "\n",
    "*What if you could decide?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try...\n",
    "\n",
    "...walking very slowly or standing still\n",
    "\n",
    "...walking backwards\n",
    "\n",
    "...starting over again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Soundfield\n",
    "\n",
    "The notes you hear in the soundfield correspond to the average global temperature for the years 1950-2100. For the years leading up to 2016, the notes are based on real temperature data from nasa.gov. The increase in frequency you hear reflects the increase the planet has experienced since 1950 (0.8 C).\n",
    "\n",
    "Future temperatures depend on our actions. Walking in the soundfield next to the red line on the left represents “business as usual” - the continuation of an economy based on fossil fuels (increase of 6 C). Walking next to the yellow line represents shifting to a clean energy future and minimizing warming by the end of the century (increase of 2 C).\n",
    "\n",
    "In the “future zone” of the soundfield, each year’s temperature is based on the temperature from the previous year; that’s why walking from the red corner to the yellow corner doesn’t result in much change. But, the soundfield lets you walk backwards in time so you can explore how different pathways from 2016 into the future affect the temperature of our planet.\n",
    "\n",
    "For more about Soundfield:  \n",
    "[bit.ly/soundfield2100](bit.ly/soundfield2100)\n",
    "\n",
    "For more about the 2100 Project:  \n",
    "[bit.ly/soundgraph](bit.ly/soundgraph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
